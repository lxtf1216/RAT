# @package _global_
defaults:
  - /data: sft/llama
  - /model/backbone: sequence
  - /model/backbone/layer@model.backbone.seq_cell: rat
  - /model/backbone/layer@model.backbone.hidden_cell: ffn
  - /model/head: lm
  - /model/embedding: lm
  - /model/embedding@model.embedding.pe: interrope
  - /task: lm
  - /optim/lr_scheduler: cosine
  - /optim/optimizer: adamw
  - _self_

data:
  _name_: narrativeqa
  global_batch_size: 32
  seq_len: 4096
  train:
    limit_tokens: -1

optim:
  optimizer:
    lr: 1.0e-5
    weight_decay: 0.01
    betas: [0.9,0.95]
  lr_scheduler:
    warmup_iter: 0.05 # keep the same with the pre-training
    eta_min: 1.0e-6

wandb:
  job_type: sft_${..data._name_}

model:
  backbone:
    num_layers: 24
    d_model: 2048
    dropout: 0.0
    bias: false
    ln: rmsnorm
    init:
      _name_: fixed
      initializer_range: 0.02
    seq_cell:
      d_head: 128
      chunk_size: 16

task:
  ignore_index: -100

trainer:
  global_batch_size: ${data.global_batch_size}
  gradient_clipping: 1.0
  max_epoch: 1
  load_checkpoint: true
  save_checkpoint: true
  save_interval: 1.0
  log_interval: 20
  eval_when_log: true
  dtype: bfloat16
  pretrained_path: /data8/zhangxin/ljc/RAT/sequence_model/exp/ckpt/sft_narrativeqa_summary/narrativeqa_summary4096-lm-lmposinterrope10000-sequenced2048l24-ratl16-ffn-lm/1e-05_128_1/510.pth
