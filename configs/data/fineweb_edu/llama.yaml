_name_: fineweb_llama # for pre-training
seq_len: 4096
ignore_input_index: 2
num_bytes: 2
vocab_size: 32000
batch_size: 1
global_batch_size: 32
train:
  _name_: lm_random
  tokenized_file_path: "/data8/zhangxin/ljc/RAT/datasets_tokenized/fineweb_edu_100B/llama-train.bin"
  batch_size: ${..batch_size}
  global_batch_size: ${..global_batch_size}
  seq_len: ${..seq_len}
  num_bytes: ${..num_bytes}
  vocab_size: ${..vocab_size}
  limit_tokens: ${eval:"110*10**9"}
  ignore_input_index: ${..ignore_input_index}
val:
  _name_: lm_random
  tokenized_file_path: "/data8/zhangxin/ljc/RAT/datasets_tokenized/fineweb_edu_100B/llama-val.bin"
  batch_size: ${..batch_size}
  global_batch_size: ${..global_batch_size}
  seq_len: ${..seq_len}
  num_bytes: ${..num_bytes}
  vocab_size: ${..vocab_size}
  ignore_input_index: ${..ignore_input_index}
  limit_tokens: ${eval:"512000*1024"}  # 0.5B tokens
